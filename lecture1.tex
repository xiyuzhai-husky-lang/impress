%:
\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{mathrsfs}
\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{eg}{Example}
\newtheorem*{thm}{Theorem}
\newtheorem*{corol}{Corollary}
\newtheorem{ex}{Exercise}[section]
{\theoremstyle{plain}
\newtheorem*{rmk}{Remark}
\newtheorem*{rmks}{Remarks}
\newtheorem*{lt}{Last time}
}
\newtheorem*{lem}{Lemma}
\usepackage{color}
\usepackage{CJK}
\title{Lecture Note on the Husky Project}
\author{Xiyu Zhai}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\tableofcontents
\section{Introduction}

The husky project (\url{https://github.com/ancient-software/husky}) is currently a project in computer vision, which starts five years ago even before my PhD.

(Pdf of lecture note: \url{https://github.com/ancient-software/husky/blob/main/sparks/impress/lecture1.pdf})

(Latex Source of lecture note: \url{https://github.com/ancient-software/husky/blob/main/sparks/impress/lecture1.tex})

Warning: this is work in progress. It shall take a long time to complete this note.

\subsection{Motivation}

\begin{eg}
	[Linear Classification]

	$f_0(x)=w_0^\top x$.

	This is easy.

	Just linear algebra.

	Mathematically speaking, no interesting mathematical structure is involved.
\end{eg}

Let $\Gamma$ be a set, let $X, W$ be input space and parameter space (can be $\mathbb{R}^n, \mathbb{R}^m$ respectively) and consider $F:X\times W\times\Gamma\to Y$ such that the ground truth $f_0$ is given by 
\begin{equation}
	f_0(x)= \sup_{\gamma\in \Gamma}F(x; w_0, \gamma)
\end{equation}
where fixed unknown parameter $w_0\in W$ and $F$ is easy to compute.

(It's more proper to write in dependent type: $F: (x: X)\to (w: W) \to \Gamma(x, w)$ to mean that $\Gamma$ is dependent on preceding $x,w$.)

If we only care about whether $f_0$ is above a certain threshold $v_0$, then
\begin{equation}
	f_0(x) > v_0\Leftrightarrow\exists \gamma\in \Gamma, F(x; w_0, \gamma) > v_0
\end{equation}
which can be seen as "NP"-problem.

We can make everything boolean by setting
\begin{equation}
	f_0(x) := \exists\gamma \in \Gamma, F(x;w_0, \gamma)
\end{equation}

We can make this harder by considering
\begin{equation}
	f_0(x) := \forall \gamma_1\in \Gamma, \exists \gamma_2\in \Gamma, F(x;w_0, \gamma_1, \gamma_2)
\end{equation}
and maybe even harder to include the problem of Go game.

There are two difficulties:
\begin{itemize}
	\item statistical one. $w_0$ is unknown
	\item computational one. How to get an efficient approximation of $f_0$ because its original mathematical form is not obviously computable.
\end{itemize}

When $f(\gamma_0,x,w_0)$ is $\epsilon$-continuous for any $\gamma_0, w_0$, so is $f_0$, then $f_0\in$, so we can put $f_0$ in some continuous function class, then we can apply traditional machine learning or deep learning to it.

In fact, machine learning can still be applied even if the original problem doesn't really have any statistical element, i.e. $W$ is univalent. This is basically what AlphaGo does. It's a way of approximate a function so that it's accurate with high probability.

The problem with this approach is that it's not optimal, overly complicated, leads to blackbox. For domain specific $F$, there might be better ways.


The project is now about finding these better ways for those $F$ in computer vision.

\begin{eg}
	[Mnist]
\end{eg}

\begin{eg}
	[Imagenet]

	Computer Graphics.
\end{eg}

Let's see how mathematicians handle similar conditions.

\begin{eg}
	[Group isomorphism]

	\begin{defn}[Group]
		A group is a set $G$ together with a multiplication $M: G\times G\to G$ such that
		\begin{enumerate}[(1)]
			\item identity
			\item inverse
			\item associative
		\end{enumerate}
	\end{defn}
\end{eg}

Fix a finite group $G_0$, let $X=\mathcal{G}$ be a set of finite group, let $W$ be univalent, and $\gamma$ consider
\begin{equation}
	F(x; w_0, \gamma) := \gamma: x \to G_0\text{ is a group isomorphism}
\end{equation}

todo: explain various ways to approximate $F$.

\begin{eg}
	[Topological Space]

	\begin{defn}[Topological Space]
		A topological space is a set $X$ together with a set of subsets (called open sets) $\tau$ such that
		\begin{itemize}
			\item $\emptyset, X\in \tau$
			\item $\forall U_1, U_2\in \tau$, $U_1\cap U_2\in \tau$
			\item $\forall \{U_i:i\in I\}\subset \tau$, $\bigcup_{i\in I}U_i\in \tau$
		\end{itemize}
	\end{defn}

	\begin{defn}[Cech Cohomology of Constant Real Valued Sheaf]
		Let $X$ be a topological space, for each open set $U$, let $\mathscr{F}(U)$ be the set of real valued constant functions on $U$.

		Let $\mathcal{U}$ be a finite open cover of $X$ ($\mathcal{U}$ contains a finite number of open sets and their union is $X$).

		For $I\subset \{1,\cdots,n\}$ define $U_I=\cap_{i\in I} U_i$, then the Cech complex is defined by

		\begin{equation}
			0\to \prod_{|I|=1} \mathscr{F}(U_I)\to
			\prod_{|I|=2} \mathscr{F}(U_I)\to
			\cdots \to 
			\prod_{|I|=i} \mathscr{F}(U_I)\to\cdots
		\end{equation}
	\end{defn}

	The maps are defined as follows. The map from $\mathscr{F}(U_I)\to \mathscr{F}(U_J)$ is 0 unless $I\subset J$, i.e., $J=I\cup \{j\}$. If $j$ is the $k$th element of $J$, then the map is $(-1)^{k-1}$ times the restriction map $\text{res}_{U_I,U_J}$.

	Define $H^i_{\mathcal{U}}(X, \mathscr{F})$ to be the $i$th cohomology group of the complex. (todo: explain this)

	todo: explain example

	$0\to \mathbb{R}^2\to \mathbb{R}^2\to 0$

	$H^0_{\mathcal{U}_k}=H^1_{\mathcal{U}_k}=\mathbb{R}$, which agrees with the singular cohomology.
\end{eg}

\subsection{Composition}

It currently comprises mainly of three parts (three totally different fields):
\begin{enumerate}[(1)]
	\item new algorithms and ways of feature constructions that are geometrically more appropriate for shape analysis;

	\begin{rmk}
	todo: explain what is geometrically appropriate
	\end{rmk}
	\item a new machine learning framework

	\item a new programming language that is critical for interactively implementing and improving efficient and fully interpretable models.

	\begin{rmk}
		PL is generalizable to other domains, like NLP, Theorem Proving, Robotics, RL, Computer Graphics, etc. I'm in the way of overhauling the type system to introduce Monad for NLP and Robotics and RL and Prop for theorem proving.

		As a result, this PL is an extremely ambitious project that must have all good things from languages in different domains, C++/Rust/Zig from system level programming
	\end{rmk}
\end{enumerate}

\begin{rmk}
	todo: explain why this is difficult for previous experts in the field.
\end{rmk}

\subsection{Difference}

\paragraph{Pattern Theory}
\url{https://en.wikipedia.org/wiki/Pattern_theory}

\paragraph{Traditional Computer Vision.} 
Handcrafted feature descriptors such as SIFT and SURF are fed to traditional
machine learning classification algorithms such as Support Vector Machines and k-Nearest Neighbours to solve the aforementioned CV problems.

Computation: matrix (mostly)

Biggest problem: doesn't work well for large datasets.


\paragraph{Deep Learning.}  End to end training.

Computation: matrix

Biggest problem: doesn't explain well, totally not scientific.

\paragraph{Husky.} Interactively construct features that is interpretable and efficient, first purely by hand then automate more and more in the process. Feature is no longer just a map from input to a vector space, it's a map from input to a type, which encodes information in an domain specific, interpretable and efficient way.

\url{https://en.wikipedia.org/wiki/Fock_space}

For image classification, the training process is replaced by the following iterative developing process (with labeling being part of it)

\begin{itemize}
	\item Replace the goal of finding an all-powerful classifier by finding for each class a one-vs-all classifier.

	\begin{rmk}
		When the number of classes is really large, we will use LSH(locally sensitive hashing)-like schemes. But for imagenet, this is not necessary.
	\end{rmk}
	\item For each class, try to represent the one-vs-all classifier by a cascade, i.e. a sequence  of partial classifiers together with a one-vs-all classifier in the end.

	todo: explain partial classifier
	\item Partial classifiers are first constructed by hand interactively. Constantly interact with the debugger.
\end{itemize}

The core advantages are
\begin{itemize}
	\item The training process doesn't require GPU and is inherent cheap to compute, everyone with a normal computer can do it.
	\item Agile. For any small change, the result is instantaneously updated and one can reason why it behaves that way. In contrast, in deep learning or tradition
\end{itemize}

its inference is much faster and less memory demanding because everything is interpretable so there are more chances for optimization and.

However, this process maps to a drastically different set of requirements on development tools which is too advanced for existing programming languages to handle:
\begin{itemize}
	\item Require the ability to easily write system level programming because we are not using matrices

	\begin{rmk}
		This rules out python.
	\end{rmk}
	\item Incrementally compute the features in a functional way, i.e. instant response even for imagenent

	\begin{rmk}
		This rules out every existing language, especially those requiring long compilation time like C++/Rust/Zig. But python and julia are kindof okay because they can run in a notebook environment. The only problem is that notebook is procedural, not functional, can't scale every well.
	\end{rmk}
	\item One-click visualization of one or more features (possibly restricted to a branch or a datapoint)

	\begin{rmk}
		This rules out every existing language.
	\end{rmk}
	\item Higher order functions for automation: one expr for constructing and training a model

	\begin{rmk}
		This rules out every existing language.
	\end{rmk}
	\item Automatic memory management.

	\begin{rmk}
		This rules out every existing language except Rust.
	\end{rmk}
	\item Automatic lazy value cache management across domains

	\begin{rmk}
		This rules out every existing language. Haskell has features that sounds similar, but isn't good enough.
	\end{rmk}
	\item Rigorous type system to avoid distracting programmer from the task at hand

	\begin{rmk}
		This rules out python.
	\end{rmk}
	\item Being friendly to customized compilation, with targets being possibly embedded device.

	\begin{rmk}
		This rules out python, julia.
	\end{rmk}
\end{itemize}


So that's why I feel it necessary to develop a new language.

\subsection{Future}


The future is going to be like:

\begin{itemize}
	\item in two years, imagenet is done in husky by myself, totally interpretable and as accurate and 100 times faster for inference; the development only requires a moderate computer and it doesn't need GPU for training and inference.

	Rougly speaking, it's going to be like:
		\begin{itemize}
			\item 4 months recognise husky
			\item 4 months recognise 9 more classes
			\item 4 months recognise 90 more with automation
			\item 4 months recognise 900 more with much better automation
			\item 8 months for improvement on all fronts
		\end{itemize}
	\item gains popularity because husky doesn't require GPU and is a new programming language that is much more easier to make right than python, and can publish papers because people are convinced.
	\item husky applies to Computer Graphics(GAN, etcs.), RL, Robotics
	\item I will raise funding for NLP and theorem proving, in 5 to 30 years will replace large language models (transformers, etc)
\end{itemize}

\subsection{Now}

The current state of the project is
\begin{itemize}
	\item theoretical stuffs are clear (nothing changes significantly from two years ago); mathematically I'm fully convinced that things will work. However, to be convincing for other people with less mathematical maturity, this is probably not enough. But it doesn't really matter because I'm going to be the one who can make essential contribution to the project due to the complexity of things.
	\item a minimal (barely) working version of language is there, for which I wrote many code in the last two years.
	\begin{itemize}
		\item first year work C++
		\item second year Rust
	\end{itemize}
	\item have only spent very limited time in actually making it work
	\begin{itemize}
		\item using the C++ version a model for mnist which is 97\% accuracy for half of the data 
		\item Mnist using the latest version in progress. Not in a hurry, because people aren't convinced by Mnist no matter how good the result is. Will do it when I have the mood.
		\item Imagenet in progress, 
	\end{itemize}
\begin{rmk}
	todo: explain what percentage means
\end{rmk}
\end{itemize}

\subsection{Past}

It initially is very different, the path is
\begin{itemize}
	\item deep learning theory
	\item algorithm + machine learning
	\item geometric algorithm
	\item programming language
\end{itemize}

\section{Inference Process for Image Classification}
\subsection{Domain Specific Definitions}

Given an input space $X$, a label space $Y$, an unknown function $f: X\to Y$, we want to approximate it with high accuracy.

\begin{rmk}
We can possibly collect data on demand or have everything ready at the beginning. For inference purpose, we really don't need to care.
\end{rmk}

\begin{defn}[Feature]
	A feature is map from $X$ to a type $S$.
\end{defn}

\begin{rmk}
	A type, rougly speaking, is a set with a specific computer representation for its elements.
\end{rmk}

\begin{rmk}
	In traditional computer vision, and 
\end{rmk}

\begin{rmk}
	In the current implementation, features are computed lazily and there is a global sheet caching (selectively) the values computed. The language allows custom computational implementation for different compilation targets based on the semantic information provided by the code. We will revisit this topic in greater details in later sections.
\end{rmk}

\begin{defn}[Option]
	Given a type $T$, Option T means $T\sqcup \{\text{Null}\}$ roughly speaking.
\end{defn}
\subsection{Mnist}

see \url{https://github.com/ancient-software/husky/blob/main/sparks/impress/vision_a_programming_approach.md}
\subsection{Imagenet(tentative)}

todo
\end{document}