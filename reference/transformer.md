# Transformer

Transformer feed-forward layers are key-value memories Geva et al, 2021 (Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy)

Transformer wish list (stanford NLP course)
- Fast and modular knowledge editing
- attribution and interpretability
- efficient scaling
