# Transformer

Transformer feed-forward layers are key-value memories Geva et al, 2021 (Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy)

Transformer wish list (stanford NLP course)
- Fast and modular knowledge editing. Robustly update the model N times without breaking its behavior on other tasks.
- attribution and interpretability. Trace a model's knowledge back to a particular document/training example
- efficient scaling. Increase the model's memory size by 10x without paying 10x more compute.
